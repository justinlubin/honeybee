# Architecture

This file documents the high-level architecture of this repository. Further documentation can be found in the code modules themselves.

This repository is split into three main sections:
1. The main implementation of Honeybee (in `backend`)
2. The evaluation materials (in `benchmark`)
3 The top-level helper scripts

The following sections provide additional details about these sections.

## Main implementation

The `backend/src/` directory contains all the code (written in Rust) for implementing the synthesizers that are used as part of Honeybee.

The entry point is `main.rs`. This file defines all the subcommands and command line arguments for Honeybee. The subcommands are handled by `main_handler.rs`. These files rely on `menu.rs`, which assembles all of the below components together into a variety of configurations that make up the synthesizers described in the PLDI '25 paper. These three files are a great way to see how all the pieces of the codebase work together.

The next set of files to take a look at are `pbn.rs`, `top_down.rs`, and `core.rs`. These implement the languages defined in the PLDI '25 paper:

- The `pbn.rs` file contains all the definitions from the paper relating to the fully-general version of Programming By Navigation. The `Controller` type in this file provides an interface that abstracts over the particular Programming By Navigation synthesizer and can be seen being used in the `main_handler.rs` file.
- The `top_down.rs` file defines one particular instantiation of Programming By Navigation to top-down steps. This is the instantiation that is used throughout the paper and is the only one that is implemented in the codebase (as of now). It also includes the definition of the top-down classical-constructive synthesis algorithm as well as inhabitation oracles.
- The `core.rs` file contains all the definitions for the core Honeybee syntax, such as functions (and function libraries), formulas, and expressions. Parsing, unparsing, type-checking, and evaluation for these expressions are defined in `parse.rs`, `unparse.rs`, `typecheck.rs`, and `eval.rs` respectively.

The `dl_oracle.rs` file translates problems using the definitions of `core.rs` into Datalog for use as a top-down oracle. The small Datalog IR is defined in the `datalog.rs` file and can be compiled into a variety of Datalog engine backends; for now, we only compile to [egglog](https://github.com/egraphs-good/egglog/) using `egglog.rs`.

All code related to benchmarking is found in `benchmark.rs`.

## Evaluation materials

The `suites/` subdirectory contains all of the benchmark entries, split into three suites:

- `fin` (entries with finitely many solutions),
- `inf` (entries with infinitely many solutions), and
- `scal` (entries with varying depth and breadth of search space, programmatically generated by the `generate.py` script in this directory).

Each entry (`.hb.toml` file) has 10 associated solutions that the synthesizers must find as part of the evaluation (except for some that have less than 10 total solutions). These solutions are found in the folder of the same name of the example and were programmatically-generated by randomly choosing steps via the `particular_driver.py` script (except for those that have a `MANUAL.txt` file associated with them; these could not be generated programatically, as described in the paper).

The `data/` subdirectory is a convenient location to store the raw results of the `benchmark` subcommand of the main implementation. 

The `analysis/` subdirectory contains a small Python [uv](https://docs.astral.sh/uv/) project to analyze the raw data produced by the `benchmark` subcommand. The main script is `analyze.py` (it contains its own usage documentation) and helper functions are stored in `lib.py`.

The `analysis/output/` subdirectory is a convenient location to store the results of the above `analyze.py` script.

## Top-level helper scripts

* `BUILD_DOCKER.sh` creates a Docker image (for use in the PLDI '25 Artifact Evaluation)
* `RUN_DOCKER.sh` runs this image, setting up the necessary files in the process.
* `RUN_QUICK_EVAL.sh` and `RUN_FULL_EVAL.sh` provide methods to run the paper evaluation (see `README.md` for more details).
* `RUN_OVERVIEW_EXAMPLE.sh` runs the example from the overview in the paper and serves as an example for how to call the synthesizer interactively.
* `KICK_TIRES.sh` checks to make sure all the external dependencies (Rust and uv) are installed correctly.